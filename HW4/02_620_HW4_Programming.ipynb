{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rorptd0liZH7"
   },
   "source": [
    "#__02-620 HW4 Programming - Image classification using Pytorch__\n",
    "\n",
    "In this last homework, we will learn basic deep learning. Deep learning is a fast moving research area. There is ongoing research on why it works so well, but so far the theory of deep learning is still building in progress. Due to this nature of the field, hands-on experience is the most important in deep learning.\n",
    "\n",
    "We will use Pytorch, one of the popular deep learning frameworks. If you are unfamiliar with the Pytorch, please watch the recitation video and look at the recitation material, where we covered basic understanding of Pytorch.\n",
    "\n",
    "Because our course is an introductory machine learning course, we covered broad topics in machine learning and thus we can't go very deep into deep learning. So we will go through only the basics of deep learning. Throughout the homework, we hope you'll get familiar with deep learning and Pytorch framework.\n",
    "\n",
    "Referenced 16-720 and 16-824, great courses in CMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DuH7q_3miVYT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x253fed5e990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feel free to import any required library you need.\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, RandomRotation, Normalize\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "torch.manual_seed(0) #For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTXZHyHSiXPC"
   },
   "source": [
    "#__General Task description__\n",
    "\n",
    "In this assignment, our task is to build a workflow of classifying the objects using deep learning models. The dataset we are interested in is CIFAR10(https://www.cs.toronto.edu/~kriz/cifar.html). This dataset includes 60,000 32x32 color images in 10 clases. It has 50,000 training samples and 10,000 test samples. You can download the dataset using torchvision.datasets.CIFAR10()\n",
    "\n",
    "__TODOs__\n",
    "1. Change path to your desired path!!!\n",
    "2. Once you download your data, you can change it to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvVmFzQ6_jZA",
    "outputId": "4a8c70a5-6e31-44ae-8ed8-b04631555026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "your_path='./data'\n",
    "download=True\n",
    "train_transform = Compose([\n",
    "    Resize((256)),\n",
    "    ToTensor(),\n",
    "    RandomRotation(degrees = (0, 180)),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize((256)),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root=your_path, train=True,download=download, transform = train_transform)\n",
    "testset = torchvision.datasets.CIFAR10(root=your_path, train=False, download=download, transform = test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hidjbg1lp62"
   },
   "source": [
    "#__Task 1 Build your own DataLoader(10 Points)__\n",
    "\n",
    "As we covered in the recitation, Pytorch uses the DataLoader class to bring datapoints to the neural network. You can do any preprocessing in this step.\n",
    "\n",
    "Take a look at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "There can be possibility that you don't have enough dataset. In this case, we can populate the dataset using data augmentation. For example, in Computer Vision, one way to augment your data is adding rotated images.\n",
    "\n",
    "BE CAREFUL: In Pytorch, one of the argument in the DataLoader is indicating train or test. This is very important argument because you do data augmentation with training samples, but not with test samples. You should consider this when you implement DataLoader.\n",
    "\n",
    "Tip:\n",
    "1. It is very important to understand what is your data and how they are organized. Observe how folder and files strutured in the downloaded CIFAR10. See the annotation files, open the images, try to see if there is any pattern in file name, etc.\n",
    "2. For data augmentation, feel free to use methods in torchvision.transforms\n",
    "\n",
    "__TODOs__\n",
    "1. Form your own DataLoader\n",
    "2. In the __getitem__, add your own data augmentation.\n",
    "3. You should have an variable that controls size of the image. For this assignment, use size of 256: that is, image should be (256, 256, 3). Please be mind that when you load this image to the Tensor, it might be changed to (3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OiO8xFxvl_T_"
   },
   "outputs": [],
   "source": [
    "# # from torch.utils.data import Dataset\n",
    "# from torchvision.io import read_image\n",
    "# #This is copied from the Pytorch document. Feel free to utilize it, or build entirely new one\n",
    "# class CustomImageDataset(Dataset):\n",
    "#     def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "#         self.img_labels = pd.read_csv(annotations_file)\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "#         image = read_image(img_path)\n",
    "#         label = self.img_labels.iloc[idx, 1]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(label)\n",
    "#         return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp4D2nRimfbs"
   },
   "source": [
    "Now you should be able to bring and use your Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5oN273lame6l"
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# root_dir = '.\\data\\cifar-10-batches-py'\n",
    "# file_path = os.path.join(root_dir, 'batches.meta')\n",
    "# with open(file_path, 'rb') as f:\n",
    "#     meta = pickle.load(f, encoding='bytes')\n",
    "\n",
    "# file_path = os.path.join(root_dir, 'data_batch_1')\n",
    "# with open(file_path, 'rb') as f:\n",
    "#     batch = pickle.load(f, encoding='bytes')\n",
    "# for key in meta:\n",
    "#     print(f'{key}: {meta[key]}')\n",
    "# print(batch.keys())\n",
    "# print(batch[b'labels'][0:10])\n",
    "# print(batch[b'data'].shape)\n",
    "# print(batch[b'filenames'])\n",
    "# class Cifar10Dataset(Dataset):\n",
    "#     def __init__(self, root):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74CqYohGnURP"
   },
   "source": [
    "#__Task 2 Build Neural Network(5 Points)__\n",
    "\n",
    "If you look at some of the deep learning papers, they provide their model architecture, which is how they organized their neural network. Through this task, you should be able to rebuild the neural network given model architecture.\n",
    "The model we are interested in is AlexNet(https://arxiv.org/abs/1404.5997). For simplicity, we are going to implement a slightly simplified version.\n",
    "\n",
    "\n",
    "This is the model you should rebuild:\n",
    "\n",
    "AlexNet(\n",
    "\n",
    "  (features): Sequential(\n",
    "  \n",
    "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "    (1): ReLU(inplace)\n",
    "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
    "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "    (4): ReLU(inplace)\n",
    "    (5): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
    "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (7): ReLU(inplace)\n",
    "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (9): ReLU(inplace)\n",
    "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (11): ReLU(inplace))\n",
    "  \n",
    "  (classifier): Sequential(\n",
    "  \n",
    "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (1): ReLU(inplace)\n",
    "    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (3): ReLU(inplace)\n",
    "    (4): Conv2d(256, 20, kernel_size=(1, 1), stride=(1, 1)))\n",
    ")\n",
    "\n",
    "\n",
    "__TODO: Implement the model architcture__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F8zVkoTmnToH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        #TODO: Define Features\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "            nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n",
    "            nn.ReLU(inplace= True),\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "            nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(inplace= True),\n",
    "            nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(inplace= True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(inplace= True)\n",
    "        )\n",
    "        \n",
    "        #TODO: Define Classifiers\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 10, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        \n",
    "        #We will give this initialization for you\n",
    "        for neuron in self.features:\n",
    "            if isinstance(neuron,nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(neuron.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #TODO: Define forward pass\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "model = AlexNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh23nNuBpp6F"
   },
   "source": [
    "#__Task 3 Build deep learning pipeline(20 Points)__\n",
    "\n",
    "You have Dataset, DataLoader, and your model. It's time to make a pipeline with ingredients. In the recitation, we covered that before we build a training loop, we need to define loss and optimizer. Due to the limited time, we will provide you with loss and optimizer. Use the given parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rl8m7gVArfFf"
   },
   "outputs": [],
   "source": [
    "#Do not change this cell\n",
    "num_iter=20\n",
    "loss_fn=nn.BCELoss()\n",
    "optimizer=torch.torch.optim.SGD(model.parameters(),lr=0.1)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWWDh9wirgx-"
   },
   "source": [
    "Now, build a deep learning pipeline. \n",
    "\n",
    "__TODOs: Finish the pipeline. Bring your DataLoader and the model here. Organize the train loop and test loop, and then train and test your model. At the end of the pipeline, your pipeline should be able to provide graphs of training accuracy, test accuracy and training loss, and report final test accuracy. If you implement it well, your test accuracy should be around 65%.__\n",
    "\n",
    "__IMPORTANT: After you get predictions from your model, please add below codes before you put the prediction into loss function. Remember below lines should be added in both training loop and test loop!__\n",
    "\n",
    "        (assume you used pred=model(X))\n",
    "        '''\n",
    "        final_layer=nn.MaxPool2d((pred.size(2),pred.size(3)))\n",
    "        pred=final_layer(pred)\n",
    "        pred=torch.reshape(pred,(-1,10))#(-1,10)\n",
    "        pred=F.sigmoid(pred)\n",
    "        '''\n",
    "        Then loss(pred,y) and goes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_one_hot = torch.nn.functional.one_hot(y, num_classes = 10)\n",
    "        y_one_hot = y_one_hot.to(device).float()\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "\n",
    "        final_layer=nn.MaxPool2d((pred.size(2),pred.size(3)))\n",
    "        pred=final_layer(pred)\n",
    "        pred=torch.reshape(pred,(-1,10))#(-1,10)\n",
    "        pred=F.sigmoid(pred)\n",
    "\n",
    "        loss = loss_fn(pred, y_one_hot)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"\\rloss: {loss:>7f}  [{current:>5d}/{size:>5d}]\", end = '')\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    correct /= size\n",
    "    return correct, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_one_hot = torch.nn.functional.one_hot(y, num_classes = 10)\n",
    "            y_one_hot = y_one_hot.to(device).float()\n",
    "            pred = model(X)\n",
    "\n",
    "            final_layer=nn.MaxPool2d((pred.size(2),pred.size(3)))\n",
    "            pred=final_layer(pred)\n",
    "            pred=torch.reshape(pred,(-1,10))#(-1,10)\n",
    "            pred=F.sigmoid(pred)\n",
    "\n",
    "            test_loss += loss_fn(pred, y_one_hot).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"\\rTest Error: \\t Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# epochs = 20\n",
    "epochs = 2\n",
    "train_accuracy, train_loss, test_accuracy, test_loss= [], [], [], []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    accuracy, loss = train(trainloader, model, loss_fn, optimizer)\n",
    "    train_accuracy.append(accuracy)\n",
    "    train_loss.append(train_loss)\n",
    "    accuracy, loss = test(testloader, model, loss_fn)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "    scheduler.step(loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_dict = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'train_loss': train_loss,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_loss': test_loss\n",
    "}\n",
    "torch.save(record_dict, '/data/train_record.pt')\n",
    "torch.save(model.state_dict(), './data/trained_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7rrmQsfs8iG"
   },
   "source": [
    "#__Task 4 Weight transfer(Transfer learning)(5 Points)__\n",
    "\n",
    "There can be two possible scenarios when you are given neural network architcture: One is build everything entirely from the scratch, as you implemented above, and the other is using the pretrained model. The former is preferred when there is no pretrained model or you are training with novel(or unpopular) dataset. The latter is generally more preferred, especially if you are working in computer vision or natural langauge processing related area, because most of the pretrained neural network works very well and their pretrained dataset is very large scale such that starting from raw training might consume great amount of time. For the latter case, it's just one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True) #DO NOT USE THIS CODE IN THIS ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will slightly tweak: You will not use the above code to bring a pretrained model. Instead, you will transfer the weights of the pretrained model to the model you built above. Knowing this will help you later when you want to build your own model but basic flow comes from previous existing models.\n",
    "\n",
    "One thing you should know, and should remember for your future is that Pytorch stores trained weights as a dictionary data structure, the key and value, and their name of the key has a pattern such that you can easily load your desired key and corresponding weights.\n",
    "\n",
    "\n",
    "If your implementation is correct, your final accuracy should be around 80%\n",
    "\n",
    "\n",
    "__TODOs: Implement below weight transfer function. Then train and test your pretrained model.Provide graphs of training accuracy, test accuracy and training loss. Report final accuracy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhjaXCUMthJO"
   },
   "outputs": [],
   "source": [
    "from torchvision._internally_replaced_utils import load_state_dict_from_url # This is the hint!\n",
    "\n",
    "def Load_Alexnet_Weight_Transferred(pretrained=True, **kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model_urls = {\n",
    "            'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
    "    }    \n",
    "    \n",
    "    model_2 = AlexNet().to(device)\n",
    "    #TODO: Initialize weights correctly based on whethet it is pretrained or not\n",
    "    statedict = torch.hub.load_state_dict_from_url(model_urls['alexnet'], progress = 'True')\n",
    "    if pretrained:\n",
    "        #Your code herestatedict = model_2.state_dict()\n",
    "        for key in statedict:\n",
    "            model_2.state_dict()[key] = statedict[key]\n",
    "\n",
    "    return model_2\n",
    "\n",
    "model_pretrained = Load_Alexnet_Weight_Transferred(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "id": "ORQpheoyuLdQ",
    "outputId": "4f58d8d9-9cd6-4b7e-dee2-84071dba6dc8"
   },
   "outputs": [],
   "source": [
    "#Use your previous training and test loops, but don't forget to use above model\n",
    "epochs = 20\n",
    "loss_fn=nn.BCELoss()\n",
    "optimizer=torch.torch.optim.SGD(model_pretrained.parameters(),lr=0.1)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "train_accuracy, train_loss, test_accuracy, test_loss= [], [], [], []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    accuracy, loss = train(trainloader, model_pretrained, loss_fn, optimizer)\n",
    "    train_accuracy.append(accuracy)\n",
    "    train_loss.append(train_loss)\n",
    "    accuracy, loss= test(testloader, model_pretrained, loss_fn)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "    scheduler.step(loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_dict = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'train_loss': train_loss,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_loss': test_loss\n",
    "}\n",
    "torch.save(record_dict, '/data/pretrained_record.pt')\n",
    "torch.save(model_pretrained.state_dict(), './data/pretrained_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcFagOoi1HuU"
   },
   "source": [
    "#__Task 5 Evaluate your model(10 Points)__\n",
    "\n",
    "You have a full loop of both train and test. Let's see how good your model is. In the lecture, we learned precision, recall. In addition to these metrics, one way to visualize our model performance is to show a heatmap of classification result. You need to build a (# classes)x(# classes) matrix. Then, for each sample in the test set with a true label ith class, we can get jth class through your model. Then we add a value to matrix[i][j], then we normalize the matrix. By doing so, we can visually show our performance. Remember that in CIFAR10, we have 10 classes. \n",
    "\n",
    "__TODOs__\n",
    "1. Visualize the heatmap, and report which class showed most accurate, and which 'task' showed most mistakes, i.e, predicted j when the true label is i.\n",
    "2. Provide 3 cases of failed prediction with most mistakes 'task'. So you should show a total of 6 images here. Briefly write why you think the model can't predict well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhA6JV_b13QH"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL2mSuhd2IlY"
   },
   "source": [
    "#__Task 6 Create your own model(10 Points OR 10 Points + 10 Bonus Points)__\n",
    "\n",
    "You have two options, and this task is open ended. \n",
    "\n",
    "Make your own model. Name the model class with your name(e.g. YoungJeNet). Briefly describe how you organized your model architecture and pipeline, and why you proposed such architecture (feel free to take inspiration from common architectures, and please reference any tutorials/guides that you use). Feel free to use your designed DataLoader and feel free to weight shift from any different model. Also feel free to use your training and test loop.\n",
    "With your defined model, run your pipeline on a new dataset and provide graphs of training accuracy and training loss, and report final accuracy.\n",
    "\n",
    "Do NOT just bring pretrained model: Like model=some_kind_of_pretrained_model(pretrain=True)\n",
    "\n",
    "For your new dataset, choose one of the following options:\n",
    "1. (10 Points) One of the following standard datasets: CIFAR10, PASCAL VOC, CALTECH256, or ImageNet2012.\n",
    "\n",
    "2. (10 Points + 10 Bonus Points) Find a dataset of your interest in biology. We are giving bonus points here because you need to describe further about your dataset and may need to work more with Dataloader.\n",
    "\n",
    "Please be mind that if you choose 2, this should not be part of your project.\n",
    "\n",
    "Your score will be determined based on 1. Description of dataset 2. Clearly stated idea and correctly implemented the idea. 3. Not too low accuracy. Please be aware that accuracy is not the only criterion here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slI2EiM12mZI"
   },
   "outputs": [],
   "source": [
    "#Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wokhgv7d6oOL"
   },
   "source": [
    "(Option) For your career, we encourage you to upload your work on Github. Github is a repository for programmers. This repo can be useful to your future career, especially if you aim to work in a computational job: provide your Github link to recruiter/research POI. By doing so, you can show your recruiter/research POI that you have fundamental ability to work with deep learning and being able to code Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHIHJqrc4QMv"
   },
   "source": [
    "Congratulations! Now you understand the basic flow of deep learning workflow. CMU provides a variety of deep learning courses, so we recommend taking any of them if you are interested or strengthen your knowledge and skills in deep learning. If you want to learn general deep learning, consider 11-685 Introduction to Deep learning. Be careful that this course is very hectic. LTI and RI offer domain specific deep learning courses(such as Natural Language Processing, Visual Learning and Recognition). If you want to know deep learning theory, consider 10-707. Please be aware that what we've covered in this assignment is very basic: this is going to be assignment 0 for other deep learning courses. However, we believe that this assignment will work as a immigration assignment to deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
